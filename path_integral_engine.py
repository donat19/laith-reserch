"""
Path Integral Rendering Engine v3.0
====================================
–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–≤–∏–∂–æ–∫ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ç–µ–≥—Ä–∞–ª–æ–≤ –ø–æ –ø—É—Ç—è–º
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –≤–µ—Ä—Å–∏–∏ –≤ –µ–¥–∏–Ω—É—é –º–æ–¥—É–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É

–ê–≤—Ç–æ—Ä: AI Assistant
–î–∞—Ç–∞: 4 —Å–µ–Ω—Ç—è–±—Ä—è 2025
"""

import math
import time
import sys
from abc import ABC, abstractmethod
from enum import Enum
from typing import Optional, Dict, Any, Tuple
import torch
import numpy as np


class RenderingBackend(Enum):
    """–¢–∏–ø—ã backend'–æ–≤ –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞"""
    CPU_NUMPY = "cpu_numpy"
    GPU_MPS = "gpu_mps" 
    GPU_CUDA = "gpu_cuda"
    CPU_TORCH = "cpu_torch"


class RenderingMode(Enum):
    """–†–µ–∂–∏–º—ã —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞"""
    MONOCHROME = "monochrome"
    RGB_SPECTRAL = "rgb_spectral"
    RGB_STANDARD = "rgb_standard"


class QualityPreset(Enum):
    """–ü—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞"""
    PREVIEW = "preview"      # –ë—ã—Å—Ç—Ä—ã–π –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä
    INTERACTIVE = "interactive"  # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
    PRODUCTION = "production"    # –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
    RESEARCH = "research"        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ


class PathIntegralConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Path Integral —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞"""
    
    def __init__(self):
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.width: int = 512
        self.height: int = 512
        self.spp: int = 16  # Samples per pixel
        self.segments: int = 4  # –°–µ–≥–º–µ–Ω—Ç—ã –ø—É—Ç–∏
        self.fov: float = 45.0
        
        # –§–∏–∑–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.wavelengths = {
            'red': 0.650,    # –º–∏–∫—Ä–æ–º–µ—Ç—Ä—ã
            'green': 0.532,
            'blue': 0.450,
            'monochrome': 0.55
        }
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏
        self.jitter_scale: float = 0.3
        self.hit_eps: float = 0.05
        self.max_steps: int = 20
        self.max_distance: float = 50.0
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.target_fps: float = 15.0
        self.min_spp: int = 4
        self.max_spp: int = 64
        self.adaptive_quality: bool = True
        
        # –†–µ–∂–∏–º—ã
        self.rendering_mode: RenderingMode = RenderingMode.RGB_SPECTRAL
        self.backend: RenderingBackend = RenderingBackend.GPU_MPS
        
        # –°–∏—Å—Ç–µ–º–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —á–∞—Å—Ç–æ—Ç —Ñ–æ—Ç–æ–Ω–æ–≤
        self.enable_normalization: bool = False
        self.normalization_frames: int = 100
        self.normalization_strength: float = 1.0  # –°–∏–ª–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (0.0-1.0)
        
    @classmethod
    def from_preset(cls, preset: QualityPreset) -> 'PathIntegralConfig':
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–∫–∏"""
        config = cls()
        
        if preset == QualityPreset.PREVIEW:
            config.width, config.height = 256, 256
            config.spp = 4
            config.segments = 2
            config.target_fps = 30.0
            
        elif preset == QualityPreset.INTERACTIVE:
            config.width, config.height = 512, 512
            config.spp = 16
            config.segments = 4
            config.target_fps = 15.0
            
        elif preset == QualityPreset.PRODUCTION:
            config.width, config.height = 1024, 1024
            config.spp = 64
            config.segments = 8
            config.target_fps = 1.0
            config.adaptive_quality = False
            
        elif preset == QualityPreset.RESEARCH:
            config.width, config.height = 1024, 1024
            config.spp = 256
            config.segments = 16
            config.target_fps = 0.1
            config.adaptive_quality = False
            
        return config


class PhotonFrequencyMap:
    """–°–∏—Å—Ç–µ–º–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —á–∞—Å—Ç–æ—Ç –∫–æ–ª–µ–±–∞–Ω–∏–π —Ñ–æ—Ç–æ–Ω–æ–≤ —Å –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–µ–π"""
    
    def __init__(self, width: int, height: int, normalization_frames: int = 100, device=None):
        self.width = width
        self.height = height
        self.normalization_frames = normalization_frames
        self.device = device or torch.device('cpu')
        
        # –ö–∞—Ä—Ç—ã –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        self.amplitude_map = torch.zeros((height, width), dtype=torch.float32, device=self.device)
        self.frequency_map = torch.zeros((height, width), dtype=torch.float32, device=self.device)
        self.phase_map = torch.zeros((height, width), dtype=torch.float32, device=self.device)
        self.frame_count = 0
        
        # –ö–∞—Ä—Ç—ã –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –¥–ª—è RGB –∫–∞–Ω–∞–ª–æ–≤
        self.rgb_interference_maps = {
            'red': torch.zeros((height, width), dtype=torch.complex64, device=self.device),
            'green': torch.zeros((height, width), dtype=torch.complex64, device=self.device),
            'blue': torch.zeros((height, width), dtype=torch.complex64, device=self.device)
        }
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–∞—Ä—Ç–∞ –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏
        self.normalized_intensity = torch.zeros((height, width, 3), dtype=torch.float32, device=self.device)
        self.is_normalized = False
        
    def accumulate_frame(self, raw_amplitudes: Dict[str, torch.Tensor], wavelengths: Dict[str, float]):
        """–ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∞–º–ø–ª–∏—Ç—É–¥ –∏ —Ñ–∞–∑ –∑–∞ –æ–¥–∏–Ω –∫–∞–¥—Ä"""
        if self.frame_count >= self.normalization_frames:
            return
            
        # –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –∞–º–ø–ª–∏—Ç—É–¥—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–∞–ª–∞
        for channel, amplitude in raw_amplitudes.items():
            if channel in self.rgb_interference_maps:
                wavelength = wavelengths[channel]
                
                # –í—ã—á–∏—Å–ª—è–µ–º —á–∞—Å—Ç–æ—Ç—É: f = c / Œª
                c = 3e8  # —Å–∫–æ—Ä–æ—Å—Ç—å —Å–≤–µ—Ç–∞ –º/—Å
                frequency = c / (wavelength * 1e-6)  # —á–∞—Å—Ç–æ—Ç–∞ –≤ –ì—Ü
                
                # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –∞–º–ø–ª–∏—Ç—É–¥—É —Å —Ñ–∞–∑–æ–π
                phase = 2 * math.pi * frequency * self.frame_count / 60.0  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º 60 FPS
                
                # –ï—Å–ª–∏ amplitude —É–∂–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è, –±–µ—Ä–µ–º –º–æ–¥—É–ª—å –¥–ª—è –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–π —á–∞—Å—Ç–∏
                if torch.is_complex(amplitude):
                    amplitude_real = torch.abs(amplitude)
                else:
                    amplitude_real = amplitude
                
                # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–∞ —Ç–æ–º –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
                phase_tensor = torch.tensor(phase, device=amplitude.device, dtype=torch.float32)
                real_part = amplitude_real * torch.cos(phase_tensor)
                imag_part = amplitude_real * torch.sin(phase_tensor)
                complex_amplitude = torch.complex(real_part, imag_part)
                
                # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∫–∞—Ä—Ç—ã –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–∏
                complex_amplitude = complex_amplitude.to(self.rgb_interference_maps[channel].device)
                
                # –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –≤ –∫–∞—Ä—Ç–µ –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–∏
                self.rgb_interference_maps[channel] += complex_amplitude
                
        self.frame_count += 1
        
        # –ï—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ 100 –∫–∞–¥—Ä–æ–≤, –≤—ã—á–∏—Å–ª—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
        if self.frame_count == self.normalization_frames:
            self._compute_normalization()
    
    def _compute_normalization(self):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –∫–∞—Ä—Ç—ã —Å —É—á–µ—Ç–æ–º –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–∏"""
        print(f"üî¨ –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–∞—Ä—Ç—ã –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –¥–ª—è {self.normalization_frames} –∫–∞–¥—Ä–æ–≤...")
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–∞–ª–∞ –≤—ã—á–∏—Å–ª—è–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–æ–Ω–Ω—É—é –∫–∞—Ä—Ç–∏–Ω—É
        for i, (channel, complex_map) in enumerate(self.rgb_interference_maps.items()):
            # –ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å = |–∞–º–ø–ª–∏—Ç—É–¥–∞|¬≤
            intensity = torch.abs(complex_map) ** 2
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å
            max_intensity = torch.max(intensity)
            if max_intensity > 0:
                intensity = intensity / max_intensity
            
            self.normalized_intensity[:, :, i] = intensity
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é –∫–∞—Ä—Ç—É —á–∞—Å—Ç–æ—Ç (—Å—Ä–µ–¥–Ω—è—è –ø–æ –≤—Å–µ–º –∫–∞–Ω–∞–ª–∞–º)
        total_amplitude = torch.abs(
            self.rgb_interference_maps['red'] + 
            self.rgb_interference_maps['green'] + 
            self.rgb_interference_maps['blue']
        )
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ–±—â—É—é –∞–º–ø–ª–∏—Ç—É–¥—É
        max_amp = torch.max(total_amplitude)
        if max_amp > 0:
            self.amplitude_map = total_amplitude / max_amp
        
        self.is_normalized = True
        print("‚úÖ –ö–∞—Ä—Ç–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –≥–æ—Ç–æ–≤–∞!")
    
    def apply_normalization(self, raw_image: torch.Tensor) -> torch.Tensor:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é"""
        if not self.is_normalized:
            # –ï—Å–ª–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –µ—â—ë –Ω–µ –≥–æ—Ç–æ–≤–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            return raw_image
        
        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º normalized_intensity –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        normalized_intensity = self.normalized_intensity.to(raw_image.device)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–æ–Ω–Ω—É—é –∫–æ—Ä—Ä–µ–∫—Ü–∏—é
        if len(raw_image.shape) == 3:  # RGB –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            corrected_image = raw_image * normalized_intensity
        else:  # –ú–æ–Ω–æ—Ö—Ä–æ–º–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            avg_intensity = torch.mean(normalized_intensity, dim=2)
            corrected_image = raw_image * avg_intensity
        
        return corrected_image
    
    def get_interference_pattern(self, channel: str = 'combined') -> torch.Tensor:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–æ–Ω–Ω–æ–π –∫–∞—Ä—Ç–∏–Ω—ã –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏"""
        if not self.is_normalized:
            return torch.zeros((self.height, self.width))
        
        if channel == 'combined':
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏—è
            combined = (
                torch.abs(self.rgb_interference_maps['red']) +
                torch.abs(self.rgb_interference_maps['green']) +
                torch.abs(self.rgb_interference_maps['blue'])
            ) / 3.0
            return combined
        elif channel in self.rgb_interference_maps:
            return torch.abs(self.rgb_interference_maps[channel])
        else:
            return torch.zeros((self.height, self.width))
    
    def reset(self):
        """–°–±—Ä–æ—Å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–æ–≤–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏"""
        self.amplitude_map = torch.zeros_like(self.amplitude_map)
        self.frequency_map = torch.zeros_like(self.frequency_map) 
        self.phase_map = torch.zeros_like(self.phase_map)
        self.frame_count = 0
        
        for channel in self.rgb_interference_maps:
            self.rgb_interference_maps[channel] = torch.zeros_like(self.rgb_interference_maps[channel])
        
        self.normalized_intensity = torch.zeros_like(self.normalized_intensity)
        self.is_normalized = False
        print("üîÑ –ö–∞—Ä—Ç–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ —Å–±—Ä–æ—à–µ–Ω–∞")


class RenderingBackendInterface(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è backend'–æ–≤ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞"""
    
    def __init__(self, config: PathIntegralConfig):
        self.config = config
        self.device = self._setup_device()
        
    @abstractmethod
    def _setup_device(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π"""
        pass
        
    @abstractmethod
    def render_frame(self, scene_sdf, camera_data) -> Tuple[np.ndarray, float, Optional[Dict[str, torch.Tensor]]]:
        """–†–µ–Ω–¥–µ—Ä–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –∫–∞–¥—Ä–∞
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        - img_array: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        - fps: –∫–∞–¥—Ä—ã –≤ —Å–µ–∫—É–Ω–¥—É  
        - raw_amplitudes: —Å–ª–æ–≤–∞—Ä—å —Å—ã—Ä—ã—Ö –∞–º–ø–ª–∏—Ç—É–¥ –ø–æ –∫–∞–Ω–∞–ª–∞–º (–¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏)
        """
        pass
        
    @abstractmethod
    def normalize_vector(self, v):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–∞"""
        pass
        
    @abstractmethod
    def scene_distance(self, points, scene_sdf):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–æ —Å—Ü–µ–Ω—ã"""
        pass


class GPUMPSBackend(RenderingBackendInterface):
    """Backend –¥–ª—è Apple Silicon GPU (Metal Performance Shaders)"""
    
    def _setup_device(self):
        if torch.backends.mps.is_available():
            return torch.device("mps")
        else:
            return torch.device("cpu")
            
    def render_frame(self, scene_sdf, camera_data) -> Tuple[np.ndarray, float, Optional[Dict[str, torch.Tensor]]]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –¥–ª—è MPS"""
        start_time = time.time()
        
        raw_amplitudes = None
        
        if self.config.rendering_mode == RenderingMode.RGB_SPECTRAL:
            img_array, raw_amplitudes = self._render_spectral(scene_sdf, camera_data)
        else:
            img_array, raw_amplitudes = self._render_monochrome(scene_sdf, camera_data)
            
        render_time = time.time() - start_time
        fps = 1.0 / render_time if render_time > 0 else 0
        
        return img_array, fps, raw_amplitudes
        
    def _render_spectral(self, scene_sdf, camera_data):
        """–°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π RGB —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥"""
        # –°–æ–∑–¥–∞–µ–º RGB –±—É—Ñ–µ—Ä—ã
        rgb_buffer = torch.zeros(self.config.height, self.config.width, 3, device=self.device)
        raw_amplitudes = {}
        
        # –†–µ–Ω–¥–µ—Ä–∏–º –∫–∞–∂–¥—ã–π –∫–∞–Ω–∞–ª –æ—Ç–¥–µ–ª—å–Ω–æ
        for i, (color, wavelength) in enumerate(self.config.wavelengths.items()):
            if color in ['red', 'green', 'blue']:
                k = 2.0 * math.pi / wavelength
                channel_intensity, channel_amplitude = self._render_channel(scene_sdf, camera_data, k)
                rgb_buffer[:, :, i] = channel_intensity
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä—ã–µ –∞–º–ø–ª–∏—Ç—É–¥—ã –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                raw_amplitudes[color] = channel_amplitude
                
        # –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞
        rgb_buffer = torch.clamp(rgb_buffer, 0.0, 1.0)
        gamma_corrected = torch.pow(rgb_buffer, 1.0/2.2)
        return (gamma_corrected * 255.0).byte().cpu().numpy(), raw_amplitudes
        
    def _render_monochrome(self, scene_sdf, camera_data):
        """–ú–æ–Ω–æ—Ö—Ä–æ–º–Ω—ã–π —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥"""
        k = 2.0 * math.pi / self.config.wavelengths['monochrome']
        intensity, amplitude = self._render_channel(scene_sdf, camera_data, k)
        
        intensity = torch.clamp(intensity, 0.0, 1.0)
        gamma_corrected = torch.pow(intensity, 1.0/2.2)
        
        # –î–ª—è –º–æ–Ω–æ—Ö—Ä–æ–º–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∞–º–ø–ª–∏—Ç—É–¥—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–º —Å RGB
        raw_amplitudes = {'monochrome': amplitude}
        
        return (gamma_corrected * 255.0).byte().cpu().numpy(), raw_amplitudes
        
    def _render_channel(self, scene_sdf, camera_data, k_value):
        """–†–µ–Ω–¥–µ—Ä–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞"""
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ª—É—á–µ–π
        ray_dirs = self._compute_ray_directions(camera_data)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±—É—Ñ–µ—Ä–æ–≤
        intensity_buffer = torch.zeros(self.config.height, self.config.width, device=self.device)
        amplitude_buffer = torch.zeros(self.config.height, self.config.width, device=self.device, dtype=torch.complex64)
        
        # –ë–∞—Ç—á–µ–≤—ã–π —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
        batch_size = min(8, self.config.spp)
        num_batches = (self.config.spp + batch_size - 1) // batch_size
        
        for batch_idx in range(num_batches):
            batch_start = batch_idx * batch_size
            batch_end = min(batch_start + batch_size, self.config.spp)
            current_batch_size = batch_end - batch_start
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—É—Ç–µ–π
            path_positions = self._generate_paths(ray_dirs, camera_data['position'], current_batch_size)
            
            # –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –ø—É—Ç–µ–π
            batch_intensities, batch_amplitudes = self._trace_paths(path_positions, scene_sdf, k_value)
            intensity_buffer += batch_intensities.sum(dim=0)
            amplitude_buffer += batch_amplitudes.sum(dim=0)
            
        final_intensity = intensity_buffer / self.config.spp
        final_amplitude = amplitude_buffer / self.config.spp
        
        return final_intensity, final_amplitude
        
    def _compute_ray_directions(self, camera_data):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –ª—É—á–µ–π –¥–ª—è –≤—Å–µ—Ö –ø–∏–∫—Å–µ–ª–µ–π"""
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç–∫—É –ø–∏–∫—Å–µ–ª–µ–π
        aspect = self.config.width / self.config.height
        screen_h = 2.0 * math.tan(math.radians(self.config.fov) / 2.0)
        screen_w = screen_h * aspect
        
        y_coords, x_coords = torch.meshgrid(
            torch.arange(self.config.height, device=self.device, dtype=torch.float32),
            torch.arange(self.config.width, device=self.device, dtype=torch.float32),
            indexing='ij'
        )
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
        u = (x_coords + 0.5) / self.config.width - 0.5
        v = (y_coords + 0.5) / self.config.height - 0.5
        screen_u = u * screen_w
        screen_v = v * screen_h
        
        # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ª—É—á–µ–π
        cam_pos = camera_data['position']
        cam_dir = camera_data['direction']
        cam_right = camera_data['right']
        cam_up = camera_data['up']
        
        pixel_centers = (
            cam_pos + cam_dir +
            cam_right * screen_u.unsqueeze(-1) +
            cam_up * screen_v.unsqueeze(-1)
        )
        
        return self.normalize_vector(pixel_centers - cam_pos)
        
    def _generate_paths(self, ray_dirs, cam_pos, batch_size):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—É—Ç–µ–π –¥–ª—è Path Integral"""
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—É—Ç–∏
        t_vals = torch.linspace(0.5, 8.0, self.config.segments, device=self.device)
        
        # –ë–∞–∑–æ–≤—ã–µ –ø–æ–∑–∏—Ü–∏–∏
        base_positions = (
            cam_pos.unsqueeze(0).unsqueeze(0).unsqueeze(0) +
            ray_dirs.unsqueeze(0).unsqueeze(3) * 
            t_vals.unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(-1)
        )
        
        # –°–ª—É—á–∞–π–Ω—ã–µ —Å–º–µ—â–µ–Ω–∏—è
        path_offsets = torch.normal(
            0, self.config.jitter_scale,
            size=(batch_size, self.config.height, self.config.width, self.config.segments, 3),
            device=self.device
        )
        
        return base_positions + path_offsets
        
    def _trace_paths(self, path_positions, scene_sdf, k_value):
        """–¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –ø—É—Ç–µ–π –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏ —Å –∞–º–ø–ª–∏—Ç—É–¥–∞–º–∏"""
        batch_size = path_positions.shape[0]
        batch_intensities = torch.zeros(batch_size, self.config.height, self.config.width, device=self.device)
        batch_amplitudes = torch.zeros(batch_size, self.config.height, self.config.width, device=self.device, dtype=torch.complex64)
        
        for i in range(batch_size):
            positions = path_positions[i]
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ –æ–ø—Ç–∏—á–µ—Å–∫–æ–π –¥–ª–∏–Ω—ã
            min_distances = torch.full((self.config.height, self.config.width), float('inf'), device=self.device)
            optical_lengths = torch.zeros(self.config.height, self.config.width, device=self.device)
            
            for seg_idx in range(self.config.segments):
                pos = positions[:, :, seg_idx, :]
                pos_flat = pos.reshape(-1, 3)
                
                # SDF –ø—Ä–æ–≤–µ—Ä–∫–∞
                distances = scene_sdf(pos_flat).reshape(self.config.height, self.config.width)
                min_distances = torch.minimum(min_distances, distances)
                
                # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –¥–ª–∏–Ω—ã
                if seg_idx > 0:
                    prev_pos = positions[:, :, seg_idx-1, :]
                    segment_length = torch.norm(pos - prev_pos, dim=-1)
                    optical_lengths += segment_length
                    
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–æ–π –∞–º–ø–ª–∏—Ç—É–¥—ã
            hit_mask = min_distances < self.config.hit_eps
            phases = k_value * optical_lengths
            
            # –î–∏—Å–ø–µ—Ä—Å–∏–æ–Ω–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã
            wavelength = 2.0 * math.pi / k_value
            dispersion_factor = 1.0 + 0.3 * (0.55 / wavelength - 1.0)
            
            # –ó–∞—Ç—É—Ö–∞–Ω–∏–µ
            attenuation = 1.0 / (1.0 + 0.05 * optical_lengths * dispersion_factor)
            
            # –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –∞–º–ø–ª–∏—Ç—É–¥–∞ —Å —É—á–µ—Ç–æ–º —Ñ–∞–∑—ã
            complex_amplitude = attenuation.unsqueeze(-1) * torch.stack([
                torch.cos(phases), torch.sin(phases)
            ], dim=-1)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ torch.complex64
            complex_amplitude = torch.complex(complex_amplitude[..., 0], complex_amplitude[..., 1])
            
            # –§–æ–Ω–æ–≤–∞—è —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∞—è –¥–ª—è –ø—Ä–æ–º–∞—Ö–æ–≤
            background_amplitude = torch.tensor(0.01, device=self.device, dtype=torch.complex64)
            
            amplitude = torch.where(
                hit_mask,
                complex_amplitude,
                background_amplitude
            )
            
            # –ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å = |–∞–º–ø–ª–∏—Ç—É–¥–∞|¬≤
            intensity = torch.abs(amplitude) ** 2
            
            batch_intensities[i] = intensity
            batch_amplitudes[i] = amplitude
            
        return batch_intensities, batch_amplitudes
        
    def normalize_vector(self, v):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–∞ –Ω–∞ GPU"""
        return torch.nn.functional.normalize(v, dim=-1)
        
    def scene_distance(self, points, scene_sdf):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–æ —Å—Ü–µ–Ω—ã"""
        return scene_sdf(points)


class CPUNumpyBackend(RenderingBackendInterface):
    """–ü—Ä–æ—Å—Ç–æ–π CPU backend –Ω–∞ NumPy (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
    
    def _setup_device(self):
        return "cpu"
        
    def render_frame(self, scene_sdf, camera_data) -> Tuple[np.ndarray, float, Optional[Dict[str, torch.Tensor]]]:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π CPU —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥"""
        start_time = time.time()
        
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        img = np.random.rand(self.config.height, self.config.width) * 128
        img = img.astype(np.uint8)
        
        # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è raw_amplitudes
        raw_amplitudes = None
        
        render_time = time.time() - start_time
        fps = 1.0 / render_time if render_time > 0 else 0
        
        return img, fps, raw_amplitudes
        
    def normalize_vector(self, v):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–∞ –Ω–∞ CPU"""
        norm = np.linalg.norm(v, axis=-1, keepdims=True)
        return np.where(norm > 1e-8, v / norm, v)
        
    def scene_distance(self, points, scene_sdf):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–æ —Å—Ü–µ–Ω—ã –Ω–∞ CPU"""
        return scene_sdf(points)


class PathIntegralEngine:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–≤–∏–∂–∫–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞"""
    
    def __init__(self, config: Optional[PathIntegralConfig] = None):
        self.config = config or PathIntegralConfig()
        self.backend = self._create_backend()
        self.stats = {
            'frames_rendered': 0,
            'total_time': 0.0,
            'avg_fps': 0.0
        }
        
        # –°–∏—Å—Ç–µ–º–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —á–∞—Å—Ç–æ—Ç —Ñ–æ—Ç–æ–Ω–æ–≤
        self.frequency_map = None
        if self.config.enable_normalization:
            self.frequency_map = PhotonFrequencyMap(
                self.config.width, 
                self.config.height, 
                self.config.normalization_frames,
                device=self.backend.device
            )
            print(f"üî¨ –°–∏—Å—Ç–µ–º–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–∫–ª—é—á–µ–Ω–∞ ({self.config.normalization_frames} –∫–∞–¥—Ä–æ–≤)")
        
    def _create_backend(self) -> RenderingBackendInterface:
        """–°–æ–∑–¥–∞–Ω–∏–µ backend'–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if self.config.backend == RenderingBackend.GPU_MPS:
            return GPUMPSBackend(self.config)
        elif self.config.backend == RenderingBackend.CPU_NUMPY:
            return CPUNumpyBackend(self.config)
        else:
            # Fallback –Ω–∞ CPU
            return CPUNumpyBackend(self.config)
            
    def render(self, scene_sdf_func, camera_position, camera_target) -> Tuple[np.ndarray, Dict[str, Any]]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —á–∞—Å—Ç–æ—Ç"""
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∫–∞–º–µ—Ä—ã
        camera_data = self._prepare_camera_data(camera_position, camera_target)
        
        # –†–µ–Ω–¥–µ—Ä–∏–Ω–≥ –∫–∞–¥—Ä–∞
        img_array, fps, raw_amplitudes = self.backend.render_frame(scene_sdf_func, camera_data)
        
        # –°–∏—Å—Ç–µ–º–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —á–∞—Å—Ç–æ—Ç —Ñ–æ—Ç–æ–Ω–æ–≤
        if self.frequency_map is not None:
            # –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            if raw_amplitudes and not self.frequency_map.is_normalized:
                self.frequency_map.accumulate_frame(raw_amplitudes, self.config.wavelengths)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –µ—Å–ª–∏ –æ–Ω–∞ –≥–æ—Ç–æ–≤–∞
            if self.frequency_map.is_normalized:
                if isinstance(img_array, np.ndarray):
                    img_tensor = torch.from_numpy(img_array)
                else:
                    img_tensor = img_array
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–æ–Ω–Ω—É—é –∫–æ—Ä—Ä–µ–∫—Ü–∏—é
                normalized_tensor = self.frequency_map.apply_normalization(img_tensor)
                
                # –°–º–µ—à–∏–≤–∞–µ–º —Å –∏—Å—Ö–æ–¥–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –ø–æ —Å–∏–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                blend_factor = self.config.normalization_strength
                img_tensor = (1.0 - blend_factor) * img_tensor + blend_factor * normalized_tensor
                
                img_array = img_tensor.cpu().numpy() if isinstance(img_tensor, torch.Tensor) else img_tensor
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self._update_stats(fps)
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–æ–º
        if self.config.adaptive_quality:
            self._adjust_quality(fps)
            
        info = {
            'fps': fps,
            'spp': self.config.spp,
            'backend': self.config.backend.value,
            'mode': self.config.rendering_mode.value,
            'stats': self.stats.copy()
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        if self.frequency_map is not None:
            info['normalization'] = {
                'enabled': True,
                'frames_accumulated': self.frequency_map.frame_count,
                'total_frames': self.frequency_map.normalization_frames,
                'is_ready': self.frequency_map.is_normalized,
                'strength': self.config.normalization_strength
            }
        else:
            info['normalization'] = {'enabled': False}
            
        return img_array, info
        
    def _prepare_camera_data(self, position, target):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∫–∞–º–µ—Ä—ã"""
        if isinstance(position, (list, tuple)):
            position = torch.tensor(position, device=self.backend.device, dtype=torch.float32)
        if isinstance(target, (list, tuple)):
            target = torch.tensor(target, device=self.backend.device, dtype=torch.float32)
            
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –±–∞–∑–∏—Å–∞ –∫–∞–º–µ—Ä—ã
        cam_dir = self.backend.normalize_vector(target - position)
        world_up = torch.tensor([0.0, 1.0, 0.0], device=self.backend.device)
        
        cam_right = self.backend.normalize_vector(torch.linalg.cross(cam_dir, world_up))
        cam_up = torch.linalg.cross(cam_right, cam_dir)
        
        return {
            'position': position,
            'direction': cam_dir,
            'right': cam_right,
            'up': cam_up,
            'target': target
        }
        
    def _update_stats(self, fps):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞"""
        self.stats['frames_rendered'] += 1
        self.stats['total_time'] += 1.0 / fps if fps > 0 else 0
        
        if self.stats['frames_rendered'] > 0:
            self.stats['avg_fps'] = self.stats['frames_rendered'] / self.stats['total_time']
            
    def _adjust_quality(self, current_fps):
        """–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–æ–º"""
        if current_fps < self.config.target_fps * 0.8:
            # –°–Ω–∏–∂–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            self.config.spp = max(self.config.min_spp, self.config.spp - 2)
        elif current_fps > self.config.target_fps * 1.2:
            # –ü–æ–≤—ã—à–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            self.config.spp = min(self.config.max_spp, self.config.spp + 1)
            
    def set_quality_preset(self, preset: QualityPreset):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞"""
        new_config = PathIntegralConfig.from_preset(preset)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º backend –∏ —Ä–µ–∂–∏–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞
        new_config.backend = self.config.backend
        new_config.rendering_mode = self.config.rendering_mode
        
        self.config = new_config
        self.backend = self._create_backend()
        
    def switch_rendering_mode(self, mode: RenderingMode):
        """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞"""
        self.config.rendering_mode = mode
        
    def get_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–≤–∏–∂–∫–µ"""
        info = {
            'version': '3.0',
            'backend': self.config.backend.value,
            'device': str(self.backend.device),
            'rendering_mode': self.config.rendering_mode.value,
            'resolution': f"{self.config.width}x{self.config.height}",
            'spp': self.config.spp,
            'stats': self.stats
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        if self.frequency_map is not None:
            info['normalization'] = {
                'enabled': True,
                'frames_accumulated': self.frequency_map.frame_count,
                'total_frames': self.frequency_map.normalization_frames,
                'is_ready': self.frequency_map.is_normalized,
                'strength': self.config.normalization_strength
            }
        else:
            info['normalization'] = {'enabled': False}
            
        return info
    
    def enable_normalization(self, frames: int = 100, strength: float = 1.0):
        """–í–∫–ª—é—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —á–∞—Å—Ç–æ—Ç —Ñ–æ—Ç–æ–Ω–æ–≤"""
        self.config.enable_normalization = True
        self.config.normalization_frames = frames
        self.config.normalization_strength = max(0.0, min(1.0, strength))
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∫–∞—Ä—Ç—É –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç –∏–ª–∏ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if (self.frequency_map is None or 
            self.frequency_map.normalization_frames != frames):
            self.frequency_map = PhotonFrequencyMap(
                self.config.width, 
                self.config.height, 
                frames,
                device=self.backend.device
            )
        
        print(f"‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞: {frames} –∫–∞–¥—Ä–æ–≤, —Å–∏–ª–∞ {strength:.1f}")
    
    def disable_normalization(self):
        """–û—Ç–∫–ª—é—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏"""
        self.config.enable_normalization = False
        self.frequency_map = None
        print("‚ùå –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞")
    
    def reset_normalization(self):
        """–°–±—Ä–æ—Å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏"""
        if self.frequency_map is not None:
            self.frequency_map.reset()
            print("üîÑ –î–∞–Ω–Ω—ã–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–±—Ä–æ—à–µ–Ω—ã")
    
    def set_normalization_strength(self, strength: float):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–∏–ª—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (0.0-1.0)"""
        self.config.normalization_strength = max(0.0, min(1.0, strength))
        print(f"‚öôÔ∏è  –°–∏–ª–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {self.config.normalization_strength:.1f}")
    
    def get_interference_pattern(self, channel: str = 'combined') -> Optional[np.ndarray]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏–æ–Ω–Ω–æ–π –∫–∞—Ä—Ç–∏–Ω—ã –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏"""
        if self.frequency_map is not None and self.frequency_map.is_normalized:
            pattern = self.frequency_map.get_interference_pattern(channel)
            return pattern.cpu().numpy() if isinstance(pattern, torch.Tensor) else pattern
        return None
    
    def export_normalization_data(self, filepath: str):
        """–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ —Ñ–∞–π–ª"""
        if self.frequency_map is not None and self.frequency_map.is_normalized:
            data = {
                'width': self.frequency_map.width,
                'height': self.frequency_map.height,
                'frames': self.frequency_map.normalization_frames,
                'normalized_intensity': self.frequency_map.normalized_intensity.cpu().numpy(),
                'rgb_interference': {
                    channel: torch.abs(complex_map).cpu().numpy()
                    for channel, complex_map in self.frequency_map.rgb_interference_maps.items()
                }
            }
            
            import pickle
            with open(filepath, 'wb') as f:
                pickle.dump(data, f)
            print(f"üíæ –î–∞–Ω–Ω—ã–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {filepath}")
        else:
            print("‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")


# –ó–∞–≤–æ–¥—Å–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
def create_interactive_engine() -> PathIntegralEngine:
    """–°–æ–∑–¥–∞–Ω–∏–µ –¥–≤–∏–∂–∫–∞ –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
    config = PathIntegralConfig.from_preset(QualityPreset.INTERACTIVE)
    return PathIntegralEngine(config)


def create_production_engine() -> PathIntegralEngine:
    """–°–æ–∑–¥–∞–Ω–∏–µ –¥–≤–∏–∂–∫–∞ –¥–ª—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞"""
    config = PathIntegralConfig.from_preset(QualityPreset.PRODUCTION)
    return PathIntegralEngine(config)


def create_preview_engine() -> PathIntegralEngine:
    """–°–æ–∑–¥–∞–Ω–∏–µ –¥–≤–∏–∂–∫–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞"""
    config = PathIntegralConfig.from_preset(QualityPreset.PREVIEW)
    return PathIntegralEngine(config)


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    print("Path Integral Rendering Engine v3.0")
    print("====================================")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–≤–∏–∂–∫–∞
    engine = create_interactive_engine()
    print(f"–î–≤–∏–∂–æ–∫ —Å–æ–∑–¥–∞–Ω: {engine.get_info()}")
    
    # –ü—Ä–æ—Å—Ç–∞—è SDF —Å—Ü–µ–Ω–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    def simple_scene_sdf(points):
        """–ü—Ä–æ—Å—Ç–∞—è —Å—Ü–µ–Ω–∞ —Å —Ç—Ä–µ–º—è —Å—Ñ–µ—Ä–∞–º–∏"""
        if hasattr(points, 'device'):  # PyTorch tensor
            device = points.device
            # –°—Ñ–µ—Ä–∞ –≤ —Ü–µ–Ω—Ç—Ä–µ
            d1 = torch.norm(points - torch.tensor([0.0, 0.0, 0.0], device=device), dim=-1) - 1.0
            # –°—Ñ–µ—Ä–∞ —Å–ø—Ä–∞–≤–∞
            d2 = torch.norm(points - torch.tensor([2.0, 0.5, 1.0], device=device), dim=-1) - 0.7
            # –ü–ª–æ—Å–∫–æ—Å—Ç—å-–ø–æ–ª
            d3 = points[..., 1] + 1.5
            return torch.minimum(torch.minimum(d1, d2), d3)
        else:  # NumPy array
            # Fallback –¥–ª—è CPU
            return np.ones(points.shape[:-1]) * 0.5
    
    # –¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥
    try:
        img, info = engine.render(
            scene_sdf_func=simple_scene_sdf,
            camera_position=[0.0, 1.0, -3.0],
            camera_target=[0.0, 0.0, 0.0]
        )
        print(f"–ö–∞–¥—Ä –æ—Ç—Ä–µ–Ω–¥–µ—Ä–µ–Ω: {img.shape}, FPS: {info['fps']:.2f}")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞: {e}")
